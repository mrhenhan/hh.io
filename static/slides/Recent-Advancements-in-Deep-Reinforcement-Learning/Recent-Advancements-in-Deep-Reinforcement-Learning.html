<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Current Advancements in Reinforcement Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Henrik Hain Supervisor: Dipl.-Ing. D. Zimmermann" />
    <meta name="date" content="2020-06-03" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/rladies-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="Recent-Advancements-in-Deep-Reinforcement-Learning.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

class: center, middle, hide-logo



&lt;div class="my-logo-left"&gt;&lt;/div&gt;
&lt;div class="my-logo-right"&gt;&lt;/div&gt;

# Current Advancements in Deep Reinforcement Learning
### Daten in software-intensiven technischen Systemen - Modellierung - Analyse - Schutz
#### Henrik Hain&lt;br&gt;Supervisor: Dipl.-Ing. D. Zimmermann
#### Institute for Program Structures and Data Organisation
#### 2020-06-03

---
# Table of Contents

.left-column[
### Contents
]

.right-column[
### 1. Recent Breakthroughs

### 2. Reinforcement Learning &amp; Deep Learning

### 3. Deep Reinforcement Learning

### 4. Selected Advancements

### 5. Conclusion and Outlook
] 

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
class: center, middle, inverse

# Recent Breakthroughs

--

## "AI equals reinforcement learning plus deep learning." - [D. Silver]

--

## "Deep reinforcement learning is poised to revolutionalize the field of artificial intelligence [...]." - [K. Arulkuraman]

---
# Recent Breakthrough

.left-column[
### 2019 AlphaStar
]

.right-column[
.right[
&lt;img src="./img/alpha_star_video.gif" width="98%"/&gt;&lt;br&gt;
Image: &lt;a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii"&gt;Google DeepMind&lt;/a&gt;]
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

???
- Plays the RTS StarCraft II on raw visual input
- Problem Dimensions
  - Imperfect Information Game
  - Large Action Space
  - Involves Long Term Planning
- Ranks Above 99.8% of all players
- Uses Combinations of RL, Heuristical Search, ...

---
# Recent Breakthrough

.left-column[
### 2019 AlphaStar

### 2018 Rubik's Cube
]

.right-column[
.right[
&lt;img src="./img/rubics-cube.png" width="100%" /&gt;&lt;br&gt;Image: &lt;a href="https://openai.com/blog/solving-rubiks-cube/"&gt;OpenAI&lt;/a&gt;
]
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

???
- Plays Chess, Shogi and go at expert program level
- True RL successor to AlphaGo
- Problem Dimensions
  - Perfect Information Game
  - Search Space Sice
  - Position Evaluation

---
# Recent Breakthrough

.left-column[
### 2019 AlphaStar

### 2018 Rubik's Cube

### 2017 AlphaZero
]

.right-column[

.right[
&lt;img src="./img/AlphaZero.gif" width="100%" /&gt;&lt;br&gt;Image: &lt;a href="https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go"&gt;Google DeepMind&lt;/a&gt;
]
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
class: center, middle, inverse

# Reinforcement Learning &amp; Deep Learning

---
# Reinforcement Learning

.left-column[
### Overview
]

.right-column[
Perception-action loop discrete timestep interaction
.right[
&lt;img src="./img/rl_overview.svg" width="90%" /&gt;
]
Typically formalized as Markov descision processes (MPD's)
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Reinforcement Learning

.left-column[
### Overview

### Policy

]

.right-column[
A policy defines an agents behavior at a given time
.right[
&lt;img src="./img/policy.png" width="80%" /&gt;
]
Policy `\(\pi\)` maps states to a probability distribution over actions `\(\pi:S \rightarrow P(A)\)`.
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Reinforcement Learning

.left-column[
### Overview

### Policy

### Reward &amp; Value
]

.right-column[
Reinforcement learning goal `\(\rightarrow\)` reward maximization
.right[
&lt;img src="./img/student_reward.png" width="40%" /&gt;
&lt;img src="./img/student_mdp.png" width="40%" /&gt;
&lt;br&gt;Images: &lt;a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=13&amp;ved=2ahUKEwjIsKPe7ujoAhVM26QKHQG2BhwQFjAMegQIBRAB&amp;url=https%3A%2F%2Fwww.davidsilver.uk%2Fwp-content%2Fuploads%2F2020%2F03%2FMDP.pdf&amp;usg=AOvVaw2G8xS6pXLns1mo_vKVQmE3"&gt;David Silver&lt;/a&gt;
]
State value `\(\rightarrow\)` sum of discounted expected future rewards `\(R_t=\sum_{i=t}^{T}\gamma^{(i-t)}r(s_i,a_i)\)`.
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Reinforcement Learning

.left-column[
### Overview

### Policy

### Reward &amp; Value

### Exploration

]

.right-column[
Exploration vs. exploitation formalized as multi-armed bandit problem
&lt;br&gt;
&lt;img src="./img/5-armed Bandit.png" width="100%" /&gt;
&lt;br&gt;
Multi-armed bandits `\(\rightarrow\)` non-associative reinforcement learning
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Reinforcement Learning

.left-column[
### Overview

### Policy

### Reward &amp; Value

### Exploration
]

.right-column[
Balance exploration vs. exploitation `\(\rightarrow\)` `\(\epsilon\)`-greedy action selection
.right[
&lt;img src="./img/bandit_reward.png" width="95%" /&gt;
]
Select greedily with probability `\(1-\epsilon\)`, else randomly
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Reinforcement Learning

.left-column[
### Overview

### Policy

### Reward &amp; Value

### Exploration

### [Model]
]

.right-column[
[Optional] model mimicing environmental dynamics `\(\rightarrow\)` optimal control / planning
.right[
&lt;img src="./img/rl_model.png" width="55%" /&gt;
&lt;br&gt;Image: &lt;a href="https://arxiv.org/abs/1901.03737"&gt;Nathan Lambert&lt;/a&gt;
]

]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Deep Learning

.left-column[
### Why?

]

.right-column[
Deep neural networks (DNN) `\(\rightarrow\)` representation learning, function approximation, scaling
.right[
&lt;img src="./img/dl_why.png" width="85%" /&gt;&lt;br&gt;Image: &lt;a href="https://towardsdatascience.com/how-managers-should-prepare-for-deep-learning-new-values-f29a98b70bd8"&gt;Richard Hackathorn&lt;/a&gt;
]
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Deep Learning

.left-column[
### Why?

### Overview

]

.right-column[
Deep neural networks (DNN) -&gt; high flexibility + domain specific standard realizations
.right[
&lt;img src="./img/cmplx_deepnetwork.png" width="90%" /&gt;&lt;br&gt;Image: &lt;a href="https://www.researchgate.net/publication/332219159_Quantum_Machine_Learning_for_6G_Communication_Networks_State-of-the-Art_and_Vision_for_the_Future"&gt;Junaid Nawaz et al.&lt;/a&gt;
]

]

.left[



]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Deep Learning

.left-column[
### Why?

### Overview

### Elements

]

.right-column[
DNN are stacked neural network `\(\rightarrow\)` exploitating feature hierarchies 
.right[
    &lt;img src="./img/shallow_network.png" width="49%" /&gt;
    &lt;img src="./img/deep_network_features.png" width="49%" /&gt;
&lt;br&gt;Image: &lt;a href="https://www.rsipvision.com/exploring-deep-learning/"&gt;www.rsipvision.com&lt;/a&gt;

]

]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Deep Learning

.left-column[
### Why?

### Overview

### Elements

### Example

]

.right-column[
Deep Q-Network (DQN) `\(\rightarrow\)` convolutional neural network + fully connected action predictor
.right[
&lt;img src="./img/dl_cnn_dqn.jpg" width="68%" /&gt;
&lt;img src="./img/saliencymap_atari.png" width="30%" /&gt;
&lt;br&gt;Images: &lt;a href="https://www.nature.com/articles/nature14236"&gt;Mnih et al.&lt;/a&gt;
]
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
class: center, middle, inverse

# Deep Reinforcement Learning

## Areas, Research, and Applications

---
# Deep Reinforcement Learning

.left-column[
### Overview

]

.right-column[
Research areas
.right[
&lt;img src="./img/openai_mainareas.svg" width="95%" /&gt;
]
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Deep Reinforcement Learning

.left-column[
### Overview

### Areas

]

.right-column[
Citation counts / research area
.right[
&lt;img src="./img/openai_mainarea_citations.svg" width="95%" /&gt;
]
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Deep Reinforcement Learning

.left-column[
### Overview

### Areas

### Research

]

.right-column[
Research of influence - Top 5
&lt;style type="text/css"&gt;
.tg  {border-collapse:collapse;border-spacing:0;border-color:#9ABAD9;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;border-color:#9ABAD9;color:#444;background-color:#EBF5FF;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;border-color:#9ABAD9;color:#fff;background-color:#409cff;}
.tg .tg-phtq{background-color:#D2E4FC;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-f48y{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class="tg" width="100%"&gt;
  &lt;tr&gt;
    &lt;th class="tg-f48y"&gt;Year&lt;/th&gt;
    &lt;th class="tg-0pky"&gt;Cited By&lt;/th&gt;
    &lt;th class="tg-0pky"&gt;Title&lt;/th&gt;
    &lt;th class="tg-0pky"&gt;Author&lt;/th&gt;
    &lt;th class="tg-0pky"&gt;Area&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-phtq"&gt;2013&lt;/td&gt;
    &lt;td class="tg-phtq"&gt;3401&lt;/td&gt;
    &lt;td class="tg-phtq"&gt;Playing Atari with Deep Reinforcement Learning&lt;/td&gt;
    &lt;td class="tg-phtq"&gt;Mnih et al.&lt;/td&gt;
    &lt;td class="tg-phtq"&gt;Model-Free/Deep Q&lt;br&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;2015&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;2707&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;Continuous Control With Deep Reinforcement Learning&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;Lillicrap et al.&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;Model-Free/DPG&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-phtq"&gt;2016&lt;br&gt;&lt;/td&gt;
    &lt;td class="tg-phtq"&gt;2634&lt;/td&gt;
    &lt;td class="tg-phtq"&gt;Asynchronous Methods for Deep Reinforcement Learning&lt;/td&gt;
    &lt;td class="tg-phtq"&gt;Mnih et al.&lt;/td&gt;
    &lt;td class="tg-phtq"&gt;Model-Free/PG&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-0pky"&gt;2015&lt;br&gt;&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;1788&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;Trust Region Policy Optimization Algorithms&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;Schulman et al.&lt;/td&gt;
    &lt;td class="tg-0pky"&gt;Model-Free/PG&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-phtq"&gt;2015&lt;/td&gt;
    &lt;td class="tg-phtq"&gt;1599&lt;/td&gt;
    &lt;td class="tg-phtq"&gt;Deep Reinforcement Learning with Double Q-learning&lt;/td&gt;
    &lt;td class="tg-phtq"&gt;Hasselt et al.&lt;/td&gt;
    &lt;td class="tg-phtq"&gt;Model-Free/Deep Q&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
class: center, middle, inverse

# Selected Advancements

### Double Q-learning, Continuous Control, Asynchronous Methods, Proximal Policy Optimization, Model-Agnostic Meta-Learning

---
# Selected Advancements

.left-column[
### Double Q-learning
]

.right-column[
Split `\(argmax\)` operator `\(\rightarrow\)` action selection, action evaluation
.right[
&lt;img src="./img/dqn_overestimation.png" width="100%" /&gt;&lt;br&gt;Image: &lt;a href="https://arxiv.org/abs/1509.06461"&gt;Hasselt et al.&lt;/a&gt;
]
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Selected Advancements

.left-column[
### Double Q-learning

### Continuous Control
]

.right-column[
Deterministic policy gradient actor-critic approach to address continuous action spaces
.right[
&lt;img src="./img/cc_performance.png" width="100%" /&gt;&lt;br&gt;Images: &lt;a href="https://arxiv.org/abs/1509.02971"&gt;Lillicrap et al.&lt;/a&gt;
]
]


&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Selected Advancements

.left-column[
### Double Q-learning

### Continuous Control

### Asynchronous Methods
]

.right-column[
Framework enabling _superlinear_ speedup with increasing number of threads (A3C)
.right[
&lt;img src="./img/async_speed.png" width="100%" /&gt;&lt;br&gt;&lt;br&gt;&lt;img src="./img/async_stability_a3c.png" width="100%" /&gt;&lt;br&gt;Images: &lt;a href="https://arxiv.org/abs/1509.06461"&gt;Mnih et al.&lt;/a&gt;
]
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Selected Advancements

.left-column[
### Double Q-learning

### Continuous Control

### Asynchronous Methods

### Proximal Policy Optimization
]

.right-column[
Enables end-to-end training via sampling from action probailities and policy rollouts
.right[
&lt;img src="./img/ppo_algorithms.png" width="100%" /&gt;&lt;br&gt;Image: &lt;a href="https://arxiv.org/abs/1707.06347"&gt;Schulman et al.&lt;/a&gt;
]
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Selected Advancements

.left-column[
### Double Q-learning

### Continuous Control

### Asynchronous Methods

### Proximal Policy Optimization

### Meta-Learning
]

.right-column[
Achieve fast learning on new tasks by providing _good_ parameter initialization
.right[
&lt;img src="./img/maml.png" width="80%" /&gt;&lt;br&gt;Image: &lt;a href="https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/"&gt;Berkeley&lt;/a&gt;
]
]

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Conclusion and Outlook

Deep reinforcement learning is a thriving and brimming field

Many large and small advancements in quick succession

Simple, but smart, ideas seem to achieve the greatest success

Current research mostly concentrated on model-free methods

Reinforcement learning can be used beneficial in almost every area!

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
class: center, middle, inverse
# Thank You!

---
# Links &amp; References

Deepmind, Google (2019, January 24). _AlphaStar: Mastering the Real-Time Strategy Game StarCraft II._ Retrieved from [https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii).

OpenAI, (2019, October 15). _Solving Rubik's Cube with a Robot Hand._ Retrieved from [https://openai.com/blog/solving-rubiks-cube/](https://openai.com/blog/solving-rubiks-cube/).

Deepmind, Google (2018, December 6). _AlphaZero: Shedding new light on chess, shogi, and Go._ Retrieved from [https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go](https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go).

UCL Course, David Silver (2015, January 1). _Markov Decision Processes._ Retrieved from [https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf](https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf).

Lambert, Nathan et al. (2019, January 11). _Low Level Control of a Quadrotor with Deep Model-Based Reinforcement Learning._ Retrieved from [Low Level Control of a Quadrotor with Deep Model-Based Reinforcement Learning](Low Level Control of a Quadrotor with Deep Model-Based Reinforcement Learning).

Hackathorn, Richard. (2018, August 20). _How Managers Should Prepare for Deep Learning: New Values._ Retrieved from [https://towardsdatascience.com/how-managers-should-prepare-for-deep-learning-new-values-f29a98b70bd8](https://towardsdatascience.com/how-managers-should-prepare-for-deep-learning-new-values-f29a98b70bd8).

Junaid Nawaz, Syed et al. (2019, April 1). _Quantum Machine Learning for 6G Communication Networks: State-of-the-Art and Vision for the Future._ Retrieved from [https://ieeexplore.ieee.org/document/8681450](https://ieeexplore.ieee.org/document/8681450).

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

---
# Literature References

Mnih, Volodymyr et al. (2013, December 19). _Playing Atari with Deep Reinforcement Learning._ Retrieved from [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602).

Lillicrap, Timothy P. (2015, September 9). _Continuous Control With Deep Reinforcement Learning._ Retrieved from [https://arxiv.org/abs/1509.02971](https://arxiv.org/abs/1509.02971).

Mnih, Volodymyr et al. (2016, February 4). _Asynchronous Methods for Deep Reinforcement Learning._ Retrieved from [https://arxiv.org/abs/1602.01783](https://arxiv.org/abs/1602.01783).

Schulman, John et al. (2015, February 19). _Trust Region Policy Optimization Algorithms._ Retrieved from [https://arxiv.org/abs/1502.05477](https://arxiv.org/abs/1502.05477).

Hasselt, Hado van et al. (2015, September 22). _Deep Reinforcement Learning with Double Q-learning._ Retrieved from [https://arxiv.org/abs/1509.06461](https://arxiv.org/abs/1509.06461).

Finn, Chelsea et al. (2017, March 9). _Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks._ Retrieved from [https://arxiv.org/abs/1703.03400](https://arxiv.org/abs/1703.03400).

&lt;div class="my-footer"&gt;&lt;span&gt;2020-06-03 - Henrik Hain - Current Advancements in Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<style>
.logo {
  background-image: url(img/kit_logo.svg);
  background-size: contain;
  background-repeat: no-repeat;
  position: absolute;
  top: 1em;
  right: 1em;
  width: 130px;
  height: 60px;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
